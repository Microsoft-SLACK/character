{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing perspectives on gender\n",
    "\n",
    "Judith Butler has argued that gender is a performative concept, which implies an audience. But different audiences may perceive the performance in different ways. This notebook gathers a few (very tentative) experiments that try to illustrate the different conceptions of gender implicit in books by men and by women.\n",
    "\n",
    "The underlying data used here is a collection of roughly 78,000 characters from 1800 to 1999, of which about 28,000 are drawn from books written by women. This is itself a subset of a larger collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from collections import Counter\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters:  78268\n",
      "Number identified as women or girls: 39134\n",
      "Number drawn from books written by women: 28183\n"
     ]
    }
   ],
   "source": [
    "metadata = pd.read_csv('../metadata/balanced_character_subset.csv')\n",
    "timeslice = metadata[(metadata.firstpub >= 1800) & (metadata.firstpub < 2000)]\n",
    "print('Number of characters: ', len(timeslice.gender))\n",
    "print('Number identified as women or girls:', sum(timeslice.gender == 'f'))\n",
    "print('Number drawn from books written by women:', sum(timeslice.authgender == 'f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a separate script (reproduce_character_models.py), I have trained six different models on subsets of 3000 characters drawn from this larger set. Each training set is divided equally between masculine and feminine characters. Three of the training sets are drawn from books by men; three from books by women.\n",
    "\n",
    "First let's start by comparing the coefficients of these models. This is not going to be terribly rigorous, quantitatively. I just want to get a sense of a few words that tend to be used differently by men and women, so I can flesh out my observation that these models could--in principle--be considered different \"perspectives\" on gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We're going to load the features of six models, treating\n",
    "# them simply as ranked lists of words. Words at the beginning\n",
    "# of each list tend to be associated with masculine characters;\n",
    "# words toward the end tend to be associated with feminine characters.\n",
    "# We could of course use the actual coefficients instead of simple\n",
    "# ranking, but I'm not convinced that adding and subtracting\n",
    "# coefficients has a firmer mathematical foundation than\n",
    "# adding and subtracting ranks would.\n",
    "\n",
    "# In order to compare these lists, we will start by filtering out words\n",
    "# that don't appear in all six lists.\n",
    "\n",
    "rootpath = 'models/'\n",
    "masculineperspectives = []\n",
    "feminineperspectives = []\n",
    "for letter in ['A', 'B', 'C']:\n",
    "    feminineperspectives.append(rootpath + 'onlywomenwriters' + letter + '.coefs.csv')\n",
    "    masculineperspectives.append(rootpath + 'onlymalewriters' + letter + '.coefs.csv')\n",
    "\n",
    "def intersection_of_models(fpaths, mpaths):\n",
    "    paths = fpaths.extend(mpaths)\n",
    "    words = []\n",
    "    for p in fpaths:\n",
    "        thislist = []\n",
    "        with open(p, encoding = 'utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                if len(row) > 0:\n",
    "                    thislist.append(row[0])\n",
    "        words.append(thislist)\n",
    "\n",
    "    shared_features = set.intersection(set(words[0]), set(words[1]), set(words[2]),\n",
    "                                   set(words[3]), set(words[4]), set(words[5]))\n",
    "    \n",
    "    filtered_features = []\n",
    "    for i in range(6):\n",
    "        newlist = []\n",
    "        for w in words[i]:\n",
    "            if w in shared_features:\n",
    "                newlist.append(w)\n",
    "            \n",
    "        filtered_features.append(newlist)\n",
    "    \n",
    "    feminine_lists = filtered_features[0 : 3]\n",
    "    masculine_lists = filtered_features[3 : 6]\n",
    "    \n",
    "    return feminine_lists, masculine_lists\n",
    "    \n",
    "                                     \n",
    "feminine_lists, masculine_lists = intersection_of_models(feminineperspectives, masculineperspectives)\n",
    "\n",
    "# now let's create a consensus ranking for both groups of writers\n",
    "\n",
    "def get_consensus(three_lists):\n",
    "    '''\n",
    "    Given three lists, constructs a consensus ranking for each\n",
    "    word. We normalize to a 0-1 scale--not strictly necessary,\n",
    "    since all lists are the same lengths, but it may be more\n",
    "    legible than raw ranks.\n",
    "    '''\n",
    "    assert len(three_lists) == 3\n",
    "    assert len(three_lists[1]) == len(three_lists[2])\n",
    "    \n",
    "    denominator = len(three_lists[0]) * 3\n",
    "    # we multiple the denominator by three\n",
    "    # because there are going to be three lists\n",
    "    \n",
    "    sum_of_ranks = Counter()\n",
    "    for alist in three_lists:\n",
    "        for index, word in enumerate(alist):\n",
    "            sum_of_ranks[word] += index / denominator\n",
    "    \n",
    "    return sum_of_ranks\n",
    "\n",
    "feminine_rankings = get_consensus(feminine_lists)\n",
    "masculine_rankings = get_consensus(masculine_lists)\n",
    "\n",
    "# Now we're going to sort words based on the DIFFERENCE\n",
    "# between feminine and masculine perspectives. \n",
    "\n",
    "# Negative scores will be words that are strongly associated with\n",
    "# men (for women) and women (for men).\n",
    "\n",
    "# Scores near zero will be words that are around the same position\n",
    "# in both models of gender.\n",
    "\n",
    "# Strongly positive scores will be words strongly associated with\n",
    "# women (for women) and men (for men).\n",
    "\n",
    "wordrank_pairs = []\n",
    "\n",
    "for word, ranking in feminine_rankings.items():\n",
    "    if word not in masculine_rankings:\n",
    "        print(error)\n",
    "    else:\n",
    "        difference = ranking - masculine_rankings[word]\n",
    "        wordrank_pairs.append((difference, word))\n",
    "\n",
    "wordrank_pairs.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.8916876574307305, 'love'),\n",
       " (-0.710495382031906, 'attentions'),\n",
       " (-0.704450041981528, 'free'),\n",
       " (-0.6740554156171286, 'was-tell'),\n",
       " (-0.6530646515533165, 'chosen'),\n",
       " (-0.6485306465155332, 'loved'),\n",
       " (-0.6203190596137698, 'was-marry'),\n",
       " (-0.6147774979009236, 'was-held'),\n",
       " (-0.6058774139378673, 'suspicions'),\n",
       " (-0.6020151133501261, 'asked'),\n",
       " (-0.5968094038623006, 'liked'),\n",
       " (-0.5924433249370278, 'firm'),\n",
       " (-0.5867338371116709, 'patient'),\n",
       " (-0.5838790931989923, 'better'),\n",
       " (-0.5732997481108313, 'was-saw'),\n",
       " (-0.5662468513853904, 'was-forgotten'),\n",
       " (-0.5645675902602854, 'accused'),\n",
       " (-0.5583543240973972, 'gone'),\n",
       " (-0.5516372795969773, 'stroked'),\n",
       " (-0.5459277917716204, 'slept'),\n",
       " (-0.543744752308984, 'was-watching'),\n",
       " (-0.5413937867338372, 'was-mean'),\n",
       " (-0.5326616288832915, 'arms'),\n",
       " (-0.5296389588581024, 'moving'),\n",
       " (-0.5151973131821999, 'fond'),\n",
       " (-0.5126784214945426, 'past'),\n",
       " (-0.5123425692695214, 'gazed'),\n",
       " (-0.5037783375314862, 'was-seen'),\n",
       " (-0.4979009235936188, 'visited'),\n",
       " (-0.4953820319059614, 'name'),\n",
       " (-0.48816120906801, 'loves'),\n",
       " (-0.48782535684298894, 'held'),\n",
       " (-0.4864819479429051, 'was-holding'),\n",
       " (-0.4827875734676742, 'forget'),\n",
       " (-0.476910159529807, 'brought'),\n",
       " (-0.472544080604534, 'talking'),\n",
       " (-0.4695214105793451, 'drawing'),\n",
       " (-0.4651553316540723, 'was-regarded'),\n",
       " (-0.46230058774139376, 'buried'),\n",
       " (-0.4589420654911839, 'was-marrying'),\n",
       " (-0.4582703610411418, 'bear'),\n",
       " (-0.45071368597816963, 'standing'),\n",
       " (-0.4502099076406383, 'married'),\n",
       " (-0.4485306465155331, 'understood'),\n",
       " (-0.4475230898404702, 'thrown'),\n",
       " (-0.44399664147774975, 'leaving'),\n",
       " (-0.44265323257766587, 'was-touch'),\n",
       " (-0.43912678421494544, 'path'),\n",
       " (-0.4381192275398824, 'despised'),\n",
       " (-0.43660789252728804, 'consciousness')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first hundred words will be negative scores,\n",
    "# strongly associated with men (for women) and women (for men).\n",
    "\n",
    "wordrank_pairs[0: 50]\n",
    "\n",
    "# as you'll see there's a lot of courtship and\n",
    "# romance here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7509655751469353, 'spend')\n",
      "(0.634592779177162, 'jaw')\n",
      "(0.6241813602015114, 'conscience')\n",
      "(0.5890848026868178, 'account')\n",
      "(0.5669185558354324, 'chair')\n",
      "(0.5667506297229219, 'wrote')\n",
      "(0.5655751469353484, 'drove')\n",
      "(0.5608732157850546, 'sent')\n",
      "(0.5521410579345088, 'busy')\n",
      "(0.543408900083963, 'was-caught')\n",
      "(0.5424013434089001, 'endeavoured')\n",
      "(0.5365239294710327, 'was-tired')\n",
      "(0.5355163727959696, 'palm')\n",
      "(0.5353484466834592, 'thoughts')\n",
      "(0.533165407220823, 'attendants')\n",
      "(0.5323257766582704, 'chin')\n",
      "(0.5306465155331654, 'history')\n",
      "(0.5251049538203191, 'gift')\n",
      "(0.5195633921074727, 'help')\n",
      "(0.5185558354324097, 'assumed')\n",
      "(0.5109991603694374, 'attack')\n",
      "(0.5036104114189757, 'thought')\n",
      "(0.5026028547439128, 'palms')\n",
      "(0.49874055415617136, 'carried')\n",
      "(0.495549958018472, 'tried')\n",
      "(0.4953820319059613, 'was-want')\n",
      "(0.4947103274559195, 'was-treat')\n",
      "(0.49454240134340893, 'was-relieved')\n",
      "(0.4945424013434089, 'think')\n",
      "(0.48984047019311505, 'years')\n",
      "(0.4879932829554996, 'half')\n",
      "(0.4849706129303107, 'brain')\n",
      "(0.48110831234256923, 'imagination')\n",
      "(0.48110831234256923, 'committed')\n",
      "(0.4789252728799328, 'wondered')\n",
      "(0.4743912678421495, 'pursued')\n",
      "(0.47069689336691867, 'receive')\n",
      "(0.47052896725440807, 'set')\n",
      "(0.4701931150293871, 'custom')\n",
      "(0.46935348446683467, 'supposed')\n",
      "(0.46414777497900916, 'head')\n",
      "(0.4624685138539043, 'forced')\n",
      "(0.46179680940386225, 'listening')\n",
      "(0.45910999160369437, 'grabbed')\n",
      "(0.456255247691016, 'remarked')\n",
      "(0.45591939546599497, 'effort')\n",
      "(0.4554156171284635, 'was-reassured')\n",
      "(0.453568429890848, 'promised')\n",
      "(0.4475230898404703, 'was-joined')\n",
      "(0.4463476070528968, 'explain')\n"
     ]
    }
   ],
   "source": [
    "# The last hundred words will be positive scores,\n",
    "# strongly associated with women (for women) and men (for men).\n",
    "\n",
    "# To keep the most important words at the top of the list,\n",
    "# I reverse it.\n",
    "\n",
    "positive = wordrank_pairs[-50 : ]\n",
    "positive.reverse()\n",
    "for pair in positive:\n",
    "    print(pair)\n",
    "\n",
    "# Much harder to characterize, and I won't actually characterize\n",
    "# this list in the article, but between you and me, I would say \n",
    "# there's a lot of effort, endeavoring, and thinking here.\n",
    "\n",
    "# \"Jaw,\" \"chin\" and \"head\" are also interesting. Perhaps in some weird way\n",
    "# they are signs of effort? \"She set her jaw ...\" Again, I'm not going\n",
    "# to actually infer anything from that -- just idly speculating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the average similarity between models\n",
    "\n",
    "Okay. The quantitative methodology above was not super-rigorous. I was just trying to get a rough sense of a few words that have notably different gender implications for writers who are men, or women. Let's try to compare these six models a little more rigorously by looking at the predictions they make.\n",
    "\n",
    "A separate function in reproduce_character_models has already gone through all six of the models used above and applied them to a balanced_test_set comprised of 1000 characters from books by women, and 1000 characters from books by men. (The characters themselves are also equally balanced by gender.) We now compare pairs of predictions about these characters, to see whether models based on books by women agree with each other more than they agree with models based on books by men, and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57028174743687998, 0.59669847910151441, 0.61282836587295142]\n",
      "similarity among models of characters by women: 0.593548944757\n",
      "[0.67252242404799922, 0.65358399158794922, 0.67195355393313982]\n",
      "similarity among models of characters by men: 0.666111448115\n"
     ]
    }
   ],
   "source": [
    "def model_correlation(firstpath, secondpath):\n",
    "    one = pd.read_csv(firstpath, index_col = 'docid')\n",
    "    two = pd.read_csv(secondpath, index_col = 'docid')\n",
    "    justpredictions = pd.concat([one['logistic'], two['logistic']], axis=1, keys=['one', 'two'])\n",
    "    justpredictions.dropna(inplace = True)\n",
    "    r, p = pearsonr(justpredictions.one, justpredictions.two)\n",
    "    return r\n",
    "\n",
    "def compare_amongst_selves(listofpredictions):\n",
    "    r_scores = []\n",
    "    already_done = []\n",
    "    for path in listofpredictions:\n",
    "        for otherpath in listofpredictions:\n",
    "            if path == otherpath:\n",
    "                continue\n",
    "            elif (path, otherpath) in already_done:\n",
    "                continue\n",
    "            else:\n",
    "                r = model_correlation(path, otherpath)\n",
    "                r_scores.append(r)\n",
    "                already_done.append((otherpath, path))\n",
    "                # no need to compare a to b AND b to a\n",
    "    return r_scores\n",
    "\n",
    "def average_r(r_scores):\n",
    "    '''\n",
    "    Technically, you don't directly average r scores; you use a\n",
    "    Fisher's transformation into z scores first. In practice, this\n",
    "    makes only a tiny difference, but ...\n",
    "    '''\n",
    "    z_scores = []\n",
    "    for r in r_scores:\n",
    "        z = np.arctanh(r)\n",
    "        z_scores.append(z)\n",
    "    mean_z = sum(z_scores) / len(z_scores)\n",
    "    mean_r = np.tanh(mean_z)\n",
    "    return mean_r\n",
    "\n",
    "rootpath = 'predictions/'\n",
    "masculineperspectives = []\n",
    "feminineperspectives = []\n",
    "for letter in ['A', 'B', 'C']:\n",
    "    feminineperspectives.append(rootpath + 'onlywomenwriters' + letter + '.results')\n",
    "    masculineperspectives.append(rootpath + 'onlymalewriters' + letter + '.results')\n",
    "\n",
    "f_compare = compare_amongst_selves(feminineperspectives)\n",
    "print(f_compare)\n",
    "print(\"similarity among models of characters by women:\", average_r(f_compare))\n",
    "m_compare = compare_amongst_selves(masculineperspectives)\n",
    "print(m_compare)\n",
    "print(\"similarity among models of characters by men:\", average_r(m_compare))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.52090949748622484, 0.56643384953528197, 0.55423932332355386, 0.53094621866695213, 0.55722508876328036, 0.54946407323176882, 0.53756463252398334, 0.55990130469704724, 0.55846360992514921]\n",
      "similarity between pairs of models that cross\n",
      "the gender boundary:  0.548508482942\n"
     ]
    }
   ],
   "source": [
    "def compare_against_each_other(listofmasculinemodels, listoffemininemodels):\n",
    "    r_scores = []\n",
    "    \n",
    "    for m in listofmasculinemodels:\n",
    "        for f in listoffemininemodels:\n",
    "            r = model_correlation(m, f)\n",
    "            r_scores.append(r)\n",
    "            \n",
    "    return r_scores\n",
    "\n",
    "both_compared = compare_against_each_other(masculineperspectives, feminineperspectives)\n",
    "print(both_compared)\n",
    "print('similarity between pairs of models that cross')\n",
    "print('the gender boundary: ', average_r(both_compared))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conclusions\n",
    "\n",
    "So we end up with three different correlation coefficients. Models based on books by men agree with each other rather strongly; this corresponds to other evidence that men tend to write conventionally gendered characters, which are easy to sort.\n",
    "\n",
    "Models of gender based on books by women tend to vary more from one random sample to another, suggesting patterns that are not quite as clearly marked. And when we compare a model based on characters written by women to one based on characters written by men, the correlation is weakest of all. Men and women don't entirely agree about definitions of gender.\n",
    "\n",
    "I have also printed the raw scores above so you get a quick and dirty grasp of uncertainty. We're not being super-systematic about this, and we only have six models. But I think there's going to be a meaningful separation between the three comparisons we're making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
